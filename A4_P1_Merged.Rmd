---
title: "Assignment 4 - Heart rate, respiration and interpersonal coordination"
author: "Louise, Anna, Oliver & Malte"
date: "November, 2019"
output:   
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 4 - Heart rate, respiration and interpersonal coordination

Physiological data (here heart rate [variability], and respiration) are increasingly popular. Historically treated as pernicious noise to be regressed out of neuro-imaging data, there is now increasing research on how these signals tell us something important about cognition and beyond being just a signal of cognitive processes also impact them in interesting ways. Advanced sport science, and the quantified self movement (closely followed by marketing and communication) have hailed continuous physiological tracking as a powerful way to access and modify attitudes, habits, and performance. Further, as team coordination (in the military, in decision processes and organizational contexts) is more and more in focus, research has attempted to measure how interpersonal coordination between physiological systems might tell us something important about e.g. emotional and cognitive coordination. See references in the reading list for more on this.

In this assignment, you will learn to:
- collect physiological data
- pre-process physiological data (and grow further your mad R skills)
- model the continuous interdependence between two signals (using a multilevel model as proxy for a dynamical system approach)
- conservatively assess the presence of coordination between to signals in a controlled context

This assignment has two parts. The first part familiarizes you with heart rate, and respiration data and their preprocessing. The second part explores how to analyze interpersonal coordination of these signals.

These are the questions you need to be able to answer at the end of the assignment (aka that you need to submit as part of the portfolio)

1) How do you preprocess heart rate and respiration data? Describe the process. If any data needs to be excluded, list the excluded data and motivate the exclusion.

2) Do you observe interpersonal coordination in heart rate and respiration? Describe your control baseline, the method used to quantify coordination, and the statistical models used to infer whether coordination was higher than in the baseline. Report the results of the models.

3) Do you observe differences in coordination between conditions? Report the models and results.

4) Is respiration coordination a likely driver of heart rate coordination? Describe how you would test for it. Bonus points if you actually run the tests and report methods and results.

N.B. to give you a bit more data I included data from previous years (Study1, Study2 and Study 3). Note that synchronouns and turn-taking are the same across both studies, but the third condition is different: in the first year it was self-paced joint reading; in the second year it was the tv-series conversation.

## Let's get started

### Exploring physiological signals

- Choose one pair (one pair, three conditions)
- Load the logs
- Produce a plot of the participants' respiration signal and a different one of the participants' HR signal.
  N.B: remember the slides: artifacts, downsampling, scaling.
  N.B. The gridExtra::grid.arrange() function allows you to display the plots side by side. E.g. grid.arrange(plot1, plot2, plot3, ncol=3). There are also smarter packages, like cowplot and ggpubr.
- Can you eye-ball which condition if any displays more physiological coordination?

### First we read one data file and identify the procedure
- Load the file
- correctly identify all columns
- plot the data
- deal with the artifacts
- downsample the dat
- Add a column for study, group, trial and condition

```{r}
# Load the libraries
library(pacman)
p_load(tidyverse, groupdata2, patchwork)

```

```{r}
# Load the file
G1T1 <- read_csv("data/Study1_G1_T1_Synchronous.csv")
G1T2 <- read_csv("data/Study1_G1_T2_TurnTaking.csv")
G1T3 <- read_csv("data/Study1_G1_T3_SelfPaced.csv")
```

```{r}
# Plot
p1HR <- ggplot(G1T1) +
  geom_line(aes(time, HR1, color = "red"))+
  geom_line(aes(time, HR2, color = "blue"))

p1RESP <- ggplot(G1T1) +
  geom_line(aes(time, Resp1, color = "red"))+
  geom_line(aes(time, Resp2, color = "blue"))

p2HR <- ggplot(G1T2) +
  geom_line(aes(time, HR1, color = "red"))+
  geom_line(aes(time, HR2, color = "blue"))

p2RESP <- ggplot(G1T2) +
  geom_line(aes(time, Resp1, color = "red"))+
  geom_line(aes(time, Resp2, color = "blue"))

p3HR <- ggplot(G1T3) +
  geom_line(aes(time, HR1, color = "red"))+
  geom_line(aes(time, HR2, color = "blue"))

p3RESP <- ggplot(G1T3) +
  geom_line(aes(time, Resp1, color = "red"))+
  geom_line(aes(time, Resp2, color = "blue"))

#(p1HR + p1RESP) / (p2HR + p2RESP) / (p3HR + p3RESP)

```

```{r}
## Remove outliers

#Defining function to get the previous non-outlier value in the time-series
getClosestMin <- function(i, withinThresh)
{
  if(i > max(withinThresh))
  {
    return(max(withinThresh))
  }
  for(j in 1:length(withinThresh))
  {
    if(withinThresh[j] > i){
      return(withinThresh[j - 1])
    }
  }
}

getClosestMax <- function(i, withinThresh)
{
  if(i < min(withinThresh))
  {
    return(min(withinThresh))  
  }
  
  for(j in length(withinThresh):1)
  {
    if(withinThresh[j] < i)
      return(withinThresh[j + 1])
  }
}
## Remove outliers
removeOuts <- function(ts){
  OutVals <- boxplot(ts)$out #Taking all outlier values according to the IQR
  withinThresh <- which(!(ts %in% OutVals)) #Taking all the indices that are NOT outliers
  for (i in 1:length(ts)){
    if(i %% 10000 == 0){ #Progress print
      print(i)
    }
    if(!(i %in% withinThresh))
    {
      if(i < withinThresh[1]){ 
        ts[i] = ts[getClosestMax(i, withinThresh)] #If the first value is an outlier find the next non-outlier value
      }
      else if(i == length(ts)){ 
        ts[i] = ts[getClosestMin(i, withinThresh)] #If the last value is an outlier find the previous non-outlier value
      }
      else{
        ts[i] = (ts[getClosestMin(i, withinThresh)] + ts[getClosestMax(i, withinThresh)]) / 2 #If the value is an outlier take the previous and the next non-outlier value and take the mean of these
      }
    }
  }
  return(ts)
}
```

```{r}
#Loading/Creating the proper variables
T1filename <- "T1noOut.csv"
T2filename <- "T2noOut.csv"
T3filename <- "T3noOut.csv"

if (file.exists(c(T1filename, T2filename, T3filename))){
  T1noOut <- read_csv(T1filename)
  T2noOut <- read_csv(T2filename)
  T3noOut <- read_csv(T3filename)
  print(paste("Loaded files:", T1filename, T2filename, T3filename))
}else{
    T1noOut <- G1T1 %>% mutate_at(c("HR1", "HR2", "Resp1", "Resp2"), removeOuts)
    T2noOut <- G1T2 %>% mutate_at(c("HR1", "HR2", "Resp1", "Resp2"), removeOuts)
    T3noOut <- G1T3 %>% mutate_at(c("HR1", "HR2", "Resp1", "Resp2"), removeOuts)
    write_csv(T1noOut, T1filename)
    write_csv(T2noOut, T2filename)
    write_csv(T3noOut, T3filename)
}


```


```{r}
# Plot raw data againt those with the artifacts removed
pOutvsNout1 <- ggplot()+
  geom_line(data = G1T1, aes(time, HR1, color = "red"))+
  geom_line(data = T1noOut, aes(time, HR1, color = "blue"))

pOutvsNout2 <- ggplot()+
  geom_line(data = G1T1, aes(time, HR2, color = "red"))+
  geom_line(data = T1noOut, aes(time, HR2, color = "blue"))

pOutvsNout3 <- ggplot()+
  geom_line(data = G1T1, aes(time, Resp1, color = "red"))+
  geom_line(data = T1noOut, aes(time, Resp1, color = "blue"))

pOutvsNout4 <- ggplot()+
  geom_line(data = G1T1, aes(time, Resp2, color = "red"))+
  geom_line(data = T1noOut, aes(time, Resp2, color = "blue"))

pOutvsNout5 <- ggplot()+
  geom_line(data = G1T2, aes(time, HR1, color = "red"))+
  geom_line(data = T2noOut, aes(time, HR1, color = "blue"))

pOutvsNout6 <- ggplot()+
  geom_line(data = G1T2, aes(time, HR2, color = "red"))+
  geom_line(data = T2noOut, aes(time, HR2, color = "blue"))

pOutvsNout7 <- ggplot()+
  geom_line(data = G1T2, aes(time, Resp1, color = "red"))+
  geom_line(data = T2noOut, aes(time, Resp1, color = "blue"))

pOutvsNout8 <- ggplot()+
  geom_line(data = G1T2, aes(time, Resp2, color = "red"))+
  geom_line(data = T2noOut, aes(time, Resp2, color = "blue"))

pOutvsNout9 <- ggplot()+
  geom_line(data = G1T3, aes(time, HR1, color = "red"))+
  geom_line(data = T3noOut, aes(time, HR1, color = "blue"))

pOutvsNout10 <- ggplot()+
  geom_line(data = G1T3, aes(time, HR2, color = "red"))+
  geom_line(data = T3noOut, aes(time, HR2, color = "blue"))

pOutvsNout11 <- ggplot()+
  geom_line(data = G1T3, aes(time, Resp1, color = "red"))+
  geom_line(data = T3noOut, aes(time, Resp1, color = "blue"))

pOutvsNout12 <- ggplot()+
  geom_line(data = G1T3, aes(time, Resp2, color = "red"))+
  geom_line(data = T3noOut, aes(time, Resp2, color = "blue"))

# pOutvsNout1
# pOutvsNout2
# pOutvsNout3
# pOutvsNout4
# pOutvsNout5
# pOutvsNout6
# pOutvsNout7
# pOutvsNout8
# pOutvsNout9
# pOutvsNout10
# pOutvsNout11
# pOutvsNout12

```

```{r}
## Scale function

z_scale <- function(column){
  column_c <- (column - mean(column)) / sd(column)
}
```


```{r}
T1noOutS <- T1noOut %>% mutate_at(c("HR1", "HR2", "Resp1", "Resp2"), z_scale)
T2noOutS <- T2noOut %>% mutate_at(c("HR1", "HR2", "Resp1", "Resp2"), z_scale)
T3noOutS <- T3noOut %>% mutate_at(c("HR1", "HR2", "Resp1", "Resp2"), z_scale)
# Plot again to check how scaled data look like
pOutvsNout1S <- ggplot()+
  geom_line(data = T1noOutS, aes(time, HR1, color = "red"))+
  geom_line(data = T1noOut, aes(time, HR1, color = "blue"))

pOutvsNout2S <- ggplot()+
  geom_line(data = T1noOutS, aes(time, HR2, color = "red"))+
  geom_line(data = T1noOut, aes(time, HR2, color = "blue"))

pOutvsNout3S <- ggplot()+
  geom_line(data = T1noOutS, aes(time, Resp1, color = "red"))+
  geom_line(data = T1noOut, aes(time, Resp1, color = "blue"))

pOutvsNout4S <- ggplot()+
  geom_line(data = T1noOutS, aes(time, Resp2, color = "red"))+
  geom_line(data = T1noOut, aes(time, Resp2, color = "blue"))

pOutvsNout5S <- ggplot()+
  geom_line(data = T2noOutS, aes(time, HR1, color = "red"))+
  geom_line(data = T2noOut, aes(time, HR1, color = "blue"))

pOutvsNout6S <- ggplot()+
  geom_line(data = T2noOutS, aes(time, HR2, color = "red"))+
  geom_line(data = T2noOut, aes(time, HR2, color = "blue"))

pOutvsNout7S <- ggplot()+
  geom_line(data = T2noOutS, aes(time, Resp1, color = "red"))+
  geom_line(data = T2noOut, aes(time, Resp1, color = "blue"))

pOutvsNout8S <- ggplot()+
  geom_line(data = T2noOutS, aes(time, Resp2, color = "red"))+
  geom_line(data = T2noOut, aes(time, Resp2, color = "blue"))

pOutvsNout9S <- ggplot()+
  geom_line(data = T3noOutS, aes(time, HR1, color = "red"))+
  geom_line(data = T3noOut, aes(time, HR1, color = "blue"))

pOutvsNout10S <- ggplot()+
  geom_line(data = T3noOutS, aes(time, HR2, color = "red"))+
  geom_line(data = T3noOut, aes(time, HR2, color = "blue"))

pOutvsNout11S <- ggplot()+
  geom_line(data = T3noOutS, aes(time, Resp1, color = "red"))+
  geom_line(data = T3noOut, aes(time, Resp1, color = "blue"))

pOutvsNout12S <- ggplot()+
  geom_line(data = T3noOutS, aes(time, Resp2, color = "red"))+
  geom_line(data = T3noOut, aes(time, Resp2, color = "blue"))

# pOutvsNout1S
# pOutvsNout2S
# pOutvsNout3S
# pOutvsNout4S
# pOutvsNout5S
# pOutvsNout6S
# pOutvsNout7S
# pOutvsNout8S
# pOutvsNout9S
# pOutvsNout10S
# pOutvsNout11S
# pOutvsNout12S


```

```{R}
## Downsample
### This is tricky, so you can have a look at my code  (relying on Ludvig's groupdata2) if you get stuck
df1 = T1noOutS %>%
  group(n = 100, method = 'greedy') %>%
  dplyr::summarise(
    time = mean(time,na.rm=T),
    HR1 = mean(HR1,na.rm=T),
    HR2 = mean(HR2,na.rm=T),
    Resp1 = mean(Resp1,na.rm=T),
    Resp2 = mean(Resp2,na.rm=T))

df2 = T2noOutS %>%
  group(n = 100, method = 'greedy') %>%
  dplyr::summarise(
    time = mean(time,na.rm=T),
    HR1 = mean(HR1,na.rm=T),
    HR2 = mean(HR2,na.rm=T),
    Resp1 = mean(Resp1,na.rm=T),
    Resp2 = mean(Resp2,na.rm=T))

df3 = T3noOutS %>%
  group(n = 100, method = 'greedy') %>%
  dplyr::summarise(
    time = mean(time,na.rm=T),
    HR1 = mean(HR1,na.rm=T),
    HR2 = mean(HR2,na.rm=T),
    Resp1 = mean(Resp1,na.rm=T),
    Resp2 = mean(Resp2,na.rm=T))
```


```{R}
#Downsample function
downsample <- function(data){
    data <- data %>% 
      group(n = 100, method = 'greedy') %>%
        dplyr::summarise(
        time = mean(time,na.rm=T),
        HR1 = mean(HR1,na.rm=T), 
        HR2 = mean(HR2,na.rm=T),
        Resp1 = mean(Resp1,na.rm=T),
        Resp2 = mean(Resp2,na.rm=T))
    return(data)
}
```


```{R}
## Plot the downsampled data
p1 <- ggplot(data = df1) +
  geom_path(aes(time, Resp1, color = "P1")) +
  geom_path(aes(time, Resp2, color = "P2")) +
  labs(x = "time", y = "Resp") +
  theme(legend.position="bottom")

p2 <- ggplot(data = df1) +
  geom_path(aes(time, HR1, color = "P1")) +
  geom_path(aes(time, HR2, color = "P2")) +
  labs(x = "time", y = "Resp") +
  theme(legend.position="bottom")

p3 <- ggplot(data = df2) +
  geom_path(aes(time, Resp1, color = "P1")) +
  geom_path(aes(time, Resp2, color = "P2")) +
  labs(x = "time", y = "Resp") +
  theme(legend.position="bottom")

p4 <- ggplot(data = df2) +
  geom_path(aes(time, HR1, color = "P1")) +
  geom_path(aes(time, HR2, color = "P2")) +
  labs(x = "time", y = "Resp") +
  theme(legend.position="bottom")

p5 <- ggplot(data = df3) +
  geom_path(aes(time, Resp1, color = "P1")) +
  geom_path(aes(time, Resp2, color = "P2")) +
  labs(x = "time", y = "Resp") +
  theme(legend.position="bottom")

p6 <- ggplot(data = df3) +
  geom_path(aes(time, HR1, color = "P1")) +
  geom_path(aes(time, HR2, color = "P2")) +
  labs(x = "time", y = "Resp") +
  theme(legend.position="bottom")

#p1
#p2
#p3
#p4
#p5
#p6


## Now add the group, trial, condition to the cleaned up, scaled, downsampled data
## Tip the info is in the file name
df1$group <- "1"
df1$trial <- "1"
df1$condition <- "Synchronous"

df2$group <- "1"
df2$trial <- "2"
df2$condition <- "TurnTaking"

df3$group <- "1"
df3$trial <- "3"
df3$condition <- "SelfPaced"
```


```{R}
addInfo <- function(df, filename){
  values <- str_extract_all(filename, "\\d+")[[1]]
  df$group <- values[2]
  df$trial <- values[3]
  df$condition <- str_extract(filename, "(?<=_)[a-zA-Z]{2,}")
  return(df)
}
```


## Now we are ready to go to load and pre-process all files

Go through all the files (with a function passed onto map_df), check which files should be excluded, if any, and save the pre-processed time-series

A couple of tips:
- looping is oh so slow. Making a function and using Map/Map_df is your salvation.
- each study restarts the group numbering, so you should make sure to change that (e.g. 100 * Study + Group)
- you need to make sure all the data are meaningful or something has to be removed. Plotting is your friend. E.g. "Study1_G1_T1_Synchronous" has one bad respiration signal. We could replace it with NAs


```{R}
# Define a function running the loading, artifact removal, scaling, downsampling, info adding.
 
col_types <- cols(
  .groups = col_double(),
  time = col_double(),
  HR1 = col_double(),
  HR2 = col_double(),
  Resp1 = col_double(),
  Resp2 = col_double(),
  group = col_double(),
  trial = col_double(),
  condition = col_character()
)

data_preprocess <- function(filename, inputdir, outputdir){
  print(filename)
  #create output folder, will only actually do something on the first function call
  inputfile <- file.path(inputdir, filename)
  dir.create("output", showWarnings = FALSE)
  outputfile = file.path(outputdir, filename)
  #check if files already in output
  if(file.exists(outputfile)){
    data <- read_csv(outputfile, col_types = col_types)
    return(data)
  }
  #Else
  data <- read_csv(inputfile)
  data <- data
  #check for study4
  if(str_detect(filename, "Study4")){
    colnames(data)[colnames(data)=="min"] <- "time"
  }
  data <- data %>% mutate_at(c("Resp1", "Resp2", "HR1", "HR2"), removeOuts)
  data <- data %>% mutate_at(c("Resp1", "Resp2", "HR1", "HR2"), z_scale)
  data <- downsample(data)
  data <- addInfo(data, filename)
  write_csv(data, outputfile)
  return(data)
}

#  Identify all files to be read
files <- list.files("data", pattern="\\.csv$")

# Run the function on the whole dataset using map_df
output <- purrr::map_df(files, data_preprocess, "data", "output")
```


```{r}
#Making a function group consequtives
groupConsequtives <- function(al){
  #list of lists ;)
  lol <- list()
  listcoords <- which(diff(al) != 1)
  startCoord <- 1
  if(length(listcoords) == 0){
    return(list(al))
  }
  for(c in listcoords){
    newl <- list(al[startCoord:c])
    startCoord <- c + 1
    lol <- c(lol, newl)
  }
  if(length(al) > listcoords[length(listcoords)])
  {
    lol <- c(lol, list(al[(listcoords[length(listcoords)]+1):length(al)]))
  }
  return(lol)
}
```


```{R}
#We forgot to include the study numbers and because the data_preprocess function takes 13 hours to run we will extract them manually 
studynum <- str_extract(files, "(\\d)") #Extracting only the study number
listofgroups <- groupConsequtives(output$.groups) #Taking the indices where the study changes to a new one

currentidx <- 0 #baseline
output$StudyNum <- 0 #creating column
for (i in 1:length(listofgroups)){
  output[listofgroups[[i]] + currentidx,]$StudyNum <- as.numeric(studynum[i]) #taking the indices + the current index and assigning the corresponding study number to it
  currentidx <- currentidx + length(listofgroups[[i]]) #Updating the current index with the length/largest value/index of the previous list
}

```

######CHUNK TO BE DELETED
```{r}
output[which(output$StudyNum == 2 & output$trial == 0),]$group <- 10
output[which(output$StudyNum == 3 & output$trial == 0),]$group <- 10

output[which(output$StudyNum == 3 & output$group == 10 & output$condition == "Synchronous"),]$trial <- 1
output[which(output$StudyNum == 3 & output$group == 10 & output$condition == "Conversation"),]$trial <- 2
output[which(output$StudyNum == 3 & output$group == 10 & output$condition == "TurnTaking"),]$trial <- 3

output[which(output$StudyNum == 2 & output$group == 10 & output$condition == "Conversation"),]$trial <- 1
output[which(output$StudyNum == 2 & output$group == 10 & output$condition == "TurnTaking"),]$trial <- 2
output[which(output$StudyNum == 2 & output$group == 10 & output$condition == "Synchronous"),]$trial <- 3
```


```{R}
# Now we need to make sure all the data are meaningful or something has to be removed
# E.g. "Study1_G1_T1_Synchronous" has one bad respiration signal. We could replace it with NAs
output[output == "NaN"] <- NA
output <- output[-c(50185:52585),]
output$pairID <- 100*output$StudyNum+output$group
ls.str(output)
output$HR1 <- as.numeric(output$HR1)
output$HR2 <- as.numeric(output$HR2)
output$Resp1 <- as.numeric(output$Resp1)
output$Resp2 <- as.numeric(output$Resp2)
output$time <- as.numeric(output$time)
ls.str(output)

#Normalising time to seconds, taking the assumption that the large values are in milliseconds and that the low values are in minutes
output$time <- ifelse(output$time > 400, output$time / 1000, output$time)

#Making a 'time-since-the-beginning-of-the-experiment' column
output <- output %>% 
  group_by(pairID, condition) %>% 
  mutate(timesincestart = time - min(time))
```


```{R}
# plots plots plots
ggplot(data = output[which(output$pairID == "102"),])+
  geom_line(aes(timesincestart, HR1, color = condition))
#At around 75 seconds after onset we see an array of consecutively repeating data.
```


```{R}
# Removing consecutive repeats in the data with a function
testSeq <- function(ts, threshold){
  runningTot <- 0 #baseline
  sequential <- rle(ts) #rle is a function that returns the values in order and the number of repeats they have
  for(i in 1:length(sequential$lengths)){
    if(sequential$lengths[i] > 10){ 
      interval <- output[sequential$lengths[i] + runningTot,]$timesincestart - output[1 + runningTot,]$timesincestart #Creating a time interval value that logs how much time has passed since the beginning of a repeating value
      if(interval > threshold){
        ts[(1 + runningTot):(runningTot+sequential$lengths[i])] <- NA #If the value is bigger than the threshhold set the given values to NAs
      }
    }
    runningTot <- runningTot + sequential$lengths[i] #Updating baseline
  }
  if(sum(is.na(ts)) > 0.25*length(ts)){
      ts[1:length(ts)] <- NA
      
  }
  return(ts)
}
#Removing the data with a threshhold of 2 seconds, meaning that if one person has a heartrate or a respiration for more than two seconds, we assume that it is spurious data.
output <- output %>% 
  group_by(pairID, condition) %>% 
  mutate_at(c("HR1", "HR2", "Resp1", "Resp2"), testSeq, 0.5) #Running the function at all the columns

# Save the data
write_csv(output, "Baddataremoved.csv")
```



## Now we need to run some analysis

Let's start with a multilevel model that accounts for 
- stability (how each signal is autocorrelated)
- interpersonal dependence (each signal is dependent from the previous state of the other signal)

The data needs to be further prepared, so we can analyze both participants in the same model.
We need to turn the data into a long format:
- a column indicating own hr and one own respiration
- a column indicating other hr and one other respiration
- a column indicating change in hr from previous round and one in respiration

We can then run an analysis where change is a function of one's previous state (stability, see slides), and the other's previous state (coupling). Make sure to:
- set up the most interesting contrasts: how do these parameters vary by condition? which condition should be baseline?
- set up the right random effects.
- N.B. the model will be slow. Make sure it works on a subset of the data first!

Bonus question: what if we include an additional layer? Is my heart rate just adjusting to yours, or also to how much you are adjusting to mine?
- to start answering this we can add a column indicating the previous change in hr in the other and one in respiration
- we can then build on the previous models by also adding the previous change in the other


```{r}
#Loading data
df <- read_csv("Baddataremoved.csv", col_types = col_types)
ls.str(df)
# Genearate a column for each: previous HR1, HR2, Resp1, Resp2
output <- df %>% 
  group_by(pairID, condition) %>% 
  mutate(HR1fut = lead(HR1,1), #lead does the same as lag
         HR2fut = lead(HR2,1), 
         Resp1fut = lead(Resp1,1), 
         Resp2fut = lead(Resp2,1),
         HR1change = HR1fut - HR1,
         HR2change = HR2fut - HR2,
         Resp1change = Resp1fut - Resp1,
         Resp2change = Resp2fut - Resp2)

# Make the data long, so we can analyze both participants at the same time 
## N.B. This is a bit tricky and you might have to do it in several steps

d_hr_change_self <- output %>% ungroup() %>% 
  gather(participant, changeHR_self, HR1change, HR2change) %>% 
  select(timesincestart, changeHR_self, participant, StudyNum, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + parse_number(participant) + StudyNum)

d_hr_change_other <- output %>% ungroup() %>% 
  gather(participant, changeHR_other, HR2change, HR1change) %>% 
  select(timesincestart, changeHR_other, participant, StudyNum, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + ifelse(parse_number(participant) == 2, 1, 2)  + StudyNum)

d_resp_change_self <- output %>% ungroup() %>% 
  gather(participant, changeResp_self, Resp1change, Resp2change) %>% 
  select(timesincestart, changeResp_self, participant, StudyNum, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + parse_number(participant)  + StudyNum)

d_resp_change_other <- output %>% ungroup() %>% 
  gather(participant, changeResp_other, Resp2change, Resp1change) %>% 
  select(timesincestart, changeResp_other, participant, StudyNum, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + ifelse(parse_number(participant) == 2, 1, 2)  + StudyNum)

d_hr_self <- output %>% ungroup() %>% 
  gather(participant, HR_self, HR1, HR2) %>% 
  select(timesincestart, HR_self, participant, StudyNum, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + parse_number(participant)  + StudyNum)

d_hr_other <- output %>% ungroup() %>% 
  gather(participant, HR_other, HR2, HR1) %>% 
  select(timesincestart, HR_other, participant, StudyNum, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + ifelse(parse_number(participant) == 2, 1, 2)  + StudyNum)

d_resp_self <- output %>% ungroup() %>% 
  gather(participant, Resp_self, Resp1, Resp2) %>% 
  select(timesincestart, Resp_self, participant, StudyNum, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + parse_number(participant)  + StudyNum)

d_resp_other <- output %>% ungroup() %>% 
  gather(participant, Resp_other, Resp2, Resp1) %>% 
  select(timesincestart, Resp_other, participant, StudyNum, group, condition, trial, pairID, time) %>% 
  mutate(participant = parse_number(as.character(pairID)) * 10 + ifelse(parse_number(participant) == 2, 1, 2)  + StudyNum)

# Merging the data
dod <- merge(d_hr_change_self, d_hr_change_other, all = T) %>% 
  merge(d_resp_change_self, all = T) %>% 
  merge(d_resp_change_other, all = T) %>% 
  merge(d_hr_self, all=T) %>% 
  merge(d_hr_other, all=T) %>% 
  merge(d_resp_self, all=T) %>% 
  merge(d_resp_other, all=T)

# Subset of data - only study 4
S4 <- dod %>% 
  filter(StudyNum == 4)
```

```{r}
# Loading package
p_load(lmerTest)

# Set the most interesting contrast e.g. by defining synchronous or conversation as the baseline
# Setting synchronous condition as the baseline
S4$condition <- relevel(as.factor(S4$condition), "Synchronous")

# Modelling
HRm1 <- lmer(changeHR_self ~ 
             0 + condition + (HR_self + HR_other) : condition + 
             0 + condition + (HR_self + HR_other) : condition + (1|participant) +
             0 + condition + (HR_self + HR_other) : condition + (1|group), data = S4, REML = F)
summary(HRm1)
# We should take the output with caution! Because of the warning! It's trapped in a 1000-dimensional space. But it is fine to do. We can also simplify - doing both or either is fine.

# Therefore, we simplify the model - removing stability and coupling as individual level parameters
HRm2 <- lmer(changeHR_self ~ 
             0 + condition + 
             0 + condition + (1|participant) +
             0 + condition + (1|group), data = S4, REML = F)
summary(HRm2)
# However, the warning is still there...

# # RESP
# Respm1 <- lmer(changeResp_self ~ 
#              (Resp_self + Resp_other) * condition + 
#              (Resp_self + Resp_other) * condition + (1|participant) +
#              (Resp_self + Resp_other) * condition + (1|group), data = S4)
# summary(Respm1)
# 
# Respm2 <- lmer(changeResp_self ~ 
#              0 + (Resp_self + Resp_other) : condition + 
#              0 + (Resp_self + Resp_other) : condition + (1|participant) +
#              0 + (Resp_self + Resp_other) : condition + (1|group), data = S4)
# summary(Respm2)



# 
HRm3 <- lmer(changeHR_self ~ 
             (HR_self + HR_other) * condition + 
             (HR_self + HR_other) * condition + (1|participant) +
             (HR_self + HR_other) * condition + (1|group), data = S4, REML = F) 
summary(HRm3)

# Bonus points: Add to the previous model also change in the other to see whether my adaptation is influenced by the other's adaptation.

```


## Now we need to create control baselines.

First shuffled controls, then surrogate pairs.

### Creating controls: shuffled controls

Shuffled controls break the temporal dependencies of time-series by shuffling the value within one time-series. This ensures the "coordination" observed is not due to the actual values in the series and not their sequence.
Tip: sample() is your friend, but make sure to shuffle things within participant/condition and not throughout the whole dataset
 
```{r}
# Setting seed
set.seed(69)

## Thank you Riccardo <3
S4 <- S4 %>%
  group_by(participant, condition) %>%
  mutate(HR_fut = lead(HR_self, 1))

shuffled_data <- S4 %>%
  group_by(participant, condition) %>%
  mutate(
    HR_self = sample(HR_self),
    HR_other = sample(HR_other),
    HR_fut = lead(HR_self,1),
    changeHR_self = HR_fut-HR_self,
    Type= "shuffled" # Defining all the shuffled data as shuffled in a new column
  )

# Creating the type column in the real data as well
S4$Type = "Real"

# Concatenating the two
d <- rbind(S4,shuffled_data)

# Create the same models as in the previous chunk, but adding an interaction by shuffled vs. real

# Setting synchronous condition as the baseline
d$condition <- relevel(as.factor(d$condition), "Synchronous")

# Modelling
HRm4 <- lmer(changeHR_self ~ 
             0 + (condition + (HR_self + HR_other) : condition) : Type + 
             0 + (condition + (HR_self + HR_other) : condition) : Type + (1|participant) +
             0 + (condition + (HR_self + HR_other) : condition) : Type + (1|group), data = d, REML = F)
summary(HRm4)
# We should take the output with caution! Because of the warning! It's trapped in a 1000-dimensional space. But it is fine to do. We can also simplify - doing both or either is fine.

# Therefore, we simplify the model - removing stability and coupling as individual level parameters
HRm5 <- lmer(changeHR_self ~ 
             0 + condition : Type + 
             0 + condition : Type + (1|participant) +
             0 + condition : Type + (1|group), data = d, REML = F)
summary(HRm5)
# However, the warning is still there...

HRm6 <- lmer(changeHR_self ~ 
             1 + (HR_self + HR_other) * condition * Type + 
             1 + (HR_self + HR_other) * condition * Type + (1|participant) +
             1 + (HR_self + HR_other) * condition * Type + (1|group), data = d, REML = F) 
summary(HRm6)

```
 
 
### TRICKY! Creating controls: surrogate pair controls
 - Per each real pair, identify at least one surrogate pair (matching one of the participants, with somebody doing the same task, but in a different pair)

```{r}
# Ungrouping the data
S4 <- S4 %>% 
  ungroup()

# Adding columns
S4$HR_other_fut <- lead(S4$HR_other, 1)
S4$HR_other_change <- S4$HR_other_fut - S4$HR_other

# Identify unique pairs within a given study (to keep things manageable) and create list of possible surrogate pairs (e.g. individual 1 from pair 1 and individual 2 from pair 2)
Groups <- as.numeric(as.character(unique(S4$group[S4$StudyNum==4]))) # Lists all pairs

SurrogateList <- expand.grid(a = Groups, b = Groups) # Identify all possible combinations of 2 pairs

SurrogateList = subset(SurrogateList, a != b) # Exclude combinations with identical pairs

surrodf <- tibble()

for (i in 1:nrow(SurrogateList)){  # loop through all combinations    
  x <- subset(S4, group==SurrogateList$a[i]) # subset data from the first pair    
  y <- subset(S4, group==SurrogateList$a[i]) # subset data from the second pair       
  group <- c(800 + ((1:4)*i))                            # create new pair id   
  for (co in c("Synchronous","TurnTaking", "Conversation", "MovementCoop", "MovementGuided")){ # loop through conditions      
    if (co %in% unique(x$condition) & co %in% unique(y$condition)){      # check that both pairs have the data for that condition         
      z1 <- subset(x, condition==co)            # subset only that condtion from first pair            
      z2 <- subset(y, condition==co)            # subset only that condtion from second pair            
      if (nrow(z1) > nrow(z2)) {	# make sure data have same length in both pairs                 
        z1<-z1[1:length(z2)]              
        }          
      if (nrow(z2) > nrow(z1)) {        
        z2<-z2[1:length(z1)]             
      }  
      w1 <- z1 %>% mutate(	# assemble new pair combining the 2 pairs                
        HR2 = z2$HR_other,              
        HR2_lead = z2$HR_self,                
        HR2_change = z2$changeHR_other)
      }  ### DO SOMETHING TO SAVE THIS	# make sure that you do this!
      surrodf <- rbind(surrodf, w1)
     }}

# Starting from the wide format, create "surrogate" dataset with the data from surrogate pairs

# Make it into long format

# Create models as in chunks above, but adding an interaction with the Real vs. Surrogate variable (exclude shuffled ones for simplicity)
```
 

### Effects of respiration coordination on heart rate coordination
 - describe how you would test those.
 - Optional: run the models and report them

 